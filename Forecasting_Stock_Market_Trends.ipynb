{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNT2EMP6XSLyxOXEbAvROHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FarazHeydar/Forecasting-Stock-Market-Trends/blob/main/Forecasting_Stock_Market_Trends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forecasting Stock Market Trends"
      ],
      "metadata": {
        "id": "S0NM60Q0vJ8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Library Imports and Data Loading and Analysis**\n",
        "In this section, we import the necessary libraries to handle financial data, visualize trends, and build machine learning models. We perform the following steps:\n",
        "1.  Data Handling: Import `pandas` and `numpy` for time-series manipulation.\n",
        "2.  Visualization: Import `seaborn` and `matplotlib` to plot market trends and heatmaps.\n",
        "3.  Machine Learning: Import `sklearn` modules for PCA, TimeSeriesSplit, and various classifiers (SVM, KNN, Random Forest) to build our predictive pipeline.\n",
        "\n",
        "Then, we download the historical stock data and prepare it for analysis. We perform the following steps:\n",
        "1.  Data Retrieval: Use `kagglehub` to fetch the Reliance 30 Years of Market Data dataset.\n",
        "2.  Date formatting: Convert the Date column to datetime objects to ensure chronological sorting.\n",
        "3.  Handling Missing Values: Apply Forward Fill (`ffill`) to propagate the last valid observation forward. This is critical in finance to simulate real-world conditions where prices remain unchanged on non-trading days, avoiding \"look-ahead bias.\""
      ],
      "metadata": {
        "id": "fswnVh1k1MY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# ==========================================\n",
        "# RAW DATA LOADING & ANALYSIS\n",
        "# ==========================================\n",
        "\n",
        "print('=' * 65)\n",
        "print(\"--- RAW DATA ANALYSIS ---\")\n",
        "print('=' * 65)\n",
        "\n",
        "# Load Data\n",
        "try:\n",
        "    print(\"Downloading dataset...\")\n",
        "    path = kagglehub.dataset_download(\"jatinkalra17/reliance-30-years-of-market-data19942025\")\n",
        "    dataset_path = f'{path}/RELIANCE_NSE_1994-2025.csv'\n",
        "    df = pd.read_csv(dataset_path)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Dataset file not found. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "# Basic Prep\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# Structural Analysis\n",
        "print(\"\\nData Structure:\")\n",
        "print(f\"\\nFirst 5 rows of the DataFrame:\\n{df.head()}\")\n",
        "\n",
        "# Missing Value Analysis\n",
        "print(\"\\nMissing Values:\")\n",
        "missing_values_count = df.isnull().sum().sum()\n",
        "print(f\"Total missing values in the dataset = {missing_values_count}\")\n",
        "\n",
        "# Handling Missing Values (Standard Financial Approach)\n",
        "# We ignore the 'Trades' column as it has too many missing values and is not needed for indicators.\n",
        "print(f\"Missing values before fill: {df[['Open', 'High', 'Low', 'Close', 'Volume']].isnull().sum().sum()}\")\n",
        "\n",
        "# Forward Fill: If a day has missing price, assume it's same as yesterday\n",
        "df = df.ffill()\n",
        "df = df.dropna() # Drop any remaining NaNs (e.g., at the very start)\n",
        "\n",
        "print(\"Data cleaned using Forward Fill.\")\n",
        "print(f\"Final Raw Shape: {df.shape}\")\n",
        "print(f\"Final Raw Head: {df.head}\")"
      ],
      "metadata": {
        "id": "k5rZ2V_8ScPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering (Technical Indicators)**\n",
        "In this section, we mathematically derive 13 technical indicators to capture market momentum, trend, and volatility. We perform the following steps:\n",
        "1.  Trend Indicators: Calculate EMA (Exponential Moving Average) and SMA to identify the general direction of the stock price.\n",
        "2.  Momentum Indicators: Compute RSI (Relative Strength Index) and MACD to detect overbought or oversold conditions.\n",
        "3.  Target Generation: Create a binary `Target` variable. We check if Tomorrow's Close > Today's Close (assigned as `1`) or otherwise (assigned as `0`), which serves as the label for our supervised learning."
      ],
      "metadata": {
        "id": "2L3PBMtbvUg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# COMPREHENSIVE FEATURE GENERATION (ALL 13 INDICATORS)\n",
        "# ==========================================\n",
        "\n",
        "print('=' * 65)\n",
        "print(\"--- GENERATING ALL 13 INDICATORS ---\")\n",
        "print('=' * 65)\n",
        "\n",
        "def calculate_all_indicators(data):\n",
        "    df = data.copy()\n",
        "\n",
        "    # 1. EMA (Exponential Moving Average)\n",
        "    # Paper Eq(1): N=14 day\n",
        "    df['EMA_14'] = df['Close'].ewm(span=14, adjust=False).mean()\n",
        "\n",
        "    # 2. MACD\n",
        "    # Paper Eq(2,3): EMA(12) - EMA(26) with Signal(9)\n",
        "    exp12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    exp26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD'] = exp12 - exp26\n",
        "\n",
        "    # 3. VTS (Volatility Stop)\n",
        "    # Paper Eq(5-8): Uses True Range (alpha)\n",
        "    # Calculating True Range (alpha)\n",
        "    high_low = df['High'] - df['Low']\n",
        "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
        "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
        "    # alpha (True Range)\n",
        "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "    # Calculate ATR (Smoothed TR) for stability over 14 days\n",
        "    atr = tr.rolling(window=14).mean()\n",
        "    # VTS = Close - (beta * alpha). Assuming beta=3 (Standard)\n",
        "    df['VTS'] = df['Close'] - (3 * atr)\n",
        "\n",
        "    # 4. T3 (Triple Exponential Moving Average)\n",
        "    # Paper cites Tim Tillson. Standard Implementation:\n",
        "    vfactor = 0.7\n",
        "    a = vfactor\n",
        "    c1 = -a**3\n",
        "    c2 = 3*a**2 + 3*a**3\n",
        "    c3 = -6*a**2 - 3*a - 3*a**3\n",
        "    c4 = 1 + 3*a + a**3 + 3*a**2\n",
        "\n",
        "    e1 = df['Close'].ewm(span=5, adjust=False).mean()\n",
        "    e2 = e1.ewm(span=5, adjust=False).mean()\n",
        "    e3 = e2.ewm(span=5, adjust=False).mean()\n",
        "    e4 = e3.ewm(span=5, adjust=False).mean()\n",
        "    e5 = e4.ewm(span=5, adjust=False).mean()\n",
        "    e6 = e5.ewm(span=5, adjust=False).mean()\n",
        "    df['T3'] = c1*e6 + c2*e5 + c3*e4 + c4*e3\n",
        "\n",
        "    # 5. Parabolic SAR (Iterative Implementation)\n",
        "    # This loop is necessary for accurate SAR calculation\n",
        "    high = df['High'].values\n",
        "    low = df['Low'].values\n",
        "    close = df['Close'].values\n",
        "    sar = np.zeros(len(df))\n",
        "\n",
        "    # Initial values\n",
        "    af = 0.02\n",
        "    max_af = 0.2\n",
        "    is_bull = True\n",
        "    sar[0] = low[0]\n",
        "    ep = high[0] # Extreme Point\n",
        "\n",
        "    for i in range(1, len(df)):\n",
        "        prev_sar = sar[i-1]\n",
        "\n",
        "        # Calculate today's SAR\n",
        "        curr_sar = prev_sar + af * (ep - prev_sar)\n",
        "\n",
        "        # Check for reversal\n",
        "        if is_bull:\n",
        "            if low[i] < curr_sar: # Reversal to Bearish\n",
        "                is_bull = False\n",
        "                curr_sar = ep\n",
        "                ep = low[i]\n",
        "                af = 0.02\n",
        "            else: # Continue Bullish\n",
        "                if high[i] > ep:\n",
        "                    ep = high[i]\n",
        "                    af = min(af + 0.02, max_af)\n",
        "                # SAR cannot be higher than the last two lows\n",
        "                curr_sar = min(curr_sar, low[max(0, i-1)], low[max(0, i-2)])\n",
        "        else:\n",
        "            if high[i] > curr_sar: # Reversal to Bullish\n",
        "                is_bull = True\n",
        "                curr_sar = ep\n",
        "                ep = high[i]\n",
        "                af = 0.02\n",
        "            else: # Continue Bearish\n",
        "                if low[i] < ep:\n",
        "                    ep = low[i]\n",
        "                    af = min(af + 0.02, max_af)\n",
        "                # SAR cannot be lower than the last two highs\n",
        "                curr_sar = max(curr_sar, high[max(0, i-1)], high[max(0, i-2)])\n",
        "\n",
        "        sar[i] = curr_sar\n",
        "\n",
        "    df['SAR'] = sar\n",
        "\n",
        "    # 6. Bollinger Bands (BB)\n",
        "    # Paper mentions \"Middle band values were used\".\n",
        "    sma20 = df['Close'].rolling(window=20).mean()\n",
        "    std20 = df['Close'].rolling(window=20).std()\n",
        "    df['BB_Middle'] = sma20 # Specifically requested by paper text\n",
        "    df['BB_Upper'] = sma20 + (2 * std20)\n",
        "    df['BB_Lower'] = sma20 - (2 * std20)\n",
        "\n",
        "    # 7. OBV (On Balance Volume)\n",
        "    # Paper: Momentum indicator based on volume flow.\n",
        "    df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).fillna(0).cumsum()\n",
        "\n",
        "    # 8. CCI (Commodity Channel Index)\n",
        "    # Paper: Detects beginning/ending trends.\n",
        "    tp = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "    sma_tp = tp.rolling(window=14).mean()\n",
        "    mean_dev = tp.rolling(window=14).apply(lambda x: np.abs(x - x.mean()).mean())\n",
        "    df['CCI'] = (tp - sma_tp) / (0.015 * mean_dev)\n",
        "\n",
        "    # 9. MTM (Momentum)\n",
        "    # Paper Eq(9): C(t) - C(-n).\n",
        "    # Table 4 Header: MTM(6_6).\n",
        "    # CRITICAL FIX: Changed from 10 to 6 to match Table 4.\n",
        "    df['MTM'] = df['Close'].diff(6)\n",
        "\n",
        "    # 10. PPO (Percentage Price Oscillator)\n",
        "    # Paper Eq(10) seems simplified , but Table 4 data  implies percentage.\n",
        "    # Header implies (26, 12, 9).\n",
        "    ema12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    df['PPO'] = ((ema12 - ema26) / ema26) * 100\n",
        "\n",
        "    # 11. PERF (Performance)\n",
        "    # Paper Eq(11): % change from start price.\n",
        "    first_price = df['Close'].iloc[0]\n",
        "    df['PERF'] = ((df['Close'] - first_price) / first_price) * 100\n",
        "\n",
        "    # 12. RSI (Added by us as \"Strong Predictor\" from analysis)\n",
        "    delta = df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # 13. SMA (Simple Moving Averages) - Included in the previous list for comparison with EMA\n",
        "    df['SMA_15'] = df['Close'].rolling(window=15).mean()\n",
        "    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
        "\n",
        "    return df\n",
        "\n",
        "df_all = calculate_all_indicators(df)\n",
        "\n",
        "# Create Target (1 if Next Day Up, 0 if Down)\n",
        "df_all['Target'] = (df_all['Close'].shift(-1) > df_all['Close']).astype(int)\n",
        "df_all = df_all.dropna() # Cleaning NaNs from indicator calculations\n",
        "\n",
        "print(\"Features Generated: EMA, MACD, VTS, T3, SAR, BB, OBV, CCI, MTM, PPO, PERF\")\n",
        "print(f\"Dataset Shape after Engineering: {df_all.shape}\")\n",
        "\n",
        "# Structural Analysis\n",
        "print(\"\\nData Structure:\")\n",
        "print(f\"\\nFirst 5 rows of the DataFrame:\\n{df_all.head()}\")\n",
        "print(f\"\\nTotal rows loaded: {len(df_all)}\")\n",
        "print(f\"\\nShape: {df_all.shape}\")\n",
        "print(f\"\\nDuplicates: {df_all.duplicated().sum()}\")\n",
        "print(f\"\\nUnique Values per Column (Content Check):\\n{df_all.nunique()}\")\n",
        "print(f\"\\nDataFrame Info (Shows structure and missing data):\\n{df_all.info()}\")\n",
        "print(f\"\\nFull Statistical Summary (Before Cleaning):\\n{df_all.describe().T}\")"
      ],
      "metadata": {
        "id": "2eCbvqTFShMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploratory Data Analysis (EDA)**\n",
        "In this section, we visualize the data to understand feature relationships and identify potential anomalies. We perform the following steps:\n",
        "1.  Correlation Heatmap: Generate a heatmap to check for multicollinearity. High correlations (e.g., between SMA_15 and EMA_14) indicate redundant features, justifying our later use of PCA.\n",
        "2.  Class Balance Inspection: Plot the distribution of the `Target` variable to ensure the dataset has a roughly equal number of \"Up\" and \"Down\" days.\n",
        "3.  Outlier Detection: Use boxplots to identify extreme values that might skew model training."
      ],
      "metadata": {
        "id": "1mFgbu831Wc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ADVANCED EDA ON ENGINEERED FEATURES\n",
        "# ==========================================\n",
        "\n",
        "print('=' * 65)\n",
        "print(\"--- ADVANCED EDA (FEATURE SELECTION) ---\")\n",
        "print('=' * 65)\n",
        "\n",
        "# List of all generated features\n",
        "indicators = ['EMA_14', 'MACD', 'VTS', 'T3', 'SAR', 'BB_Middle', 'OBV', 'CCI', 'MTM', 'PPO', 'PERF', 'RSI', 'SMA_15', 'SMA_50']\n",
        "all_features = ['Open', 'High', 'Low', 'Close', 'Volume'] + indicators\n",
        "target_col = ['Target']\n",
        "\n",
        "# Target Variable Distribution\n",
        "print(\"\\nVisualizing Target Variable Distribution...\")\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(x='Target', data=df_all, palette='viridis', hue='Target', legend=False)\n",
        "plt.title('Distribution of Target Variable\\n', fontsize=14)\n",
        "plt.xlabel('Target (0: Down, 1: Up)', fontsize=10)\n",
        "plt.ylabel('Count', fontsize=10)\n",
        "\n",
        "# Add text labels\n",
        "target_counts = df_all['Target'].value_counts().sort_index()\n",
        "for i in range(len(target_counts)):\n",
        "    plt.text(i, target_counts[i] + 100, f\"{target_counts[i]} ({target_counts[i]/len(df_all)*100:.1f}%)\", ha='center', fontsize=11)\n",
        "plt.show()\n",
        "\n",
        "# Feature Distribution (Visual Check)\n",
        "print(\"\\nVisualizing New Data Distributions...\")\n",
        "plt.suptitle('Univariate Feature Distributions', fontsize=20, y=1.02)\n",
        "for i, feature in enumerate(indicators):\n",
        "    plt.figure(figsize=(10, 5)) # Create a new figure for each plot\n",
        "    sns.histplot(df_all[feature], kde=True, bins=50)\n",
        "    plt.title(f'Distribution of {feature}', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Time Series Trends\n",
        "print(\"\\nVisualizing Time Series Trends...\")\n",
        "for i, col in enumerate(indicators):\n",
        "    plt.figure(figsize=(10, 5)) # Create a new figure for each plot\n",
        "    plt.plot(df_all['Date'], df_all[col], label=f'{col} Trend')\n",
        "    plt.title(f'[1.5] {col} Time Series Trend', fontsize=14)\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(col)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Outlier Analysis (Boxplots)\n",
        "print(\"\\nVisualizing Outlier...\")\n",
        "for i, col in enumerate(all_features):\n",
        "    plt.figure(figsize=(10, 5)) # Create a new figure for each plot\n",
        "    sns.boxplot(x=df_all[col], palette='pastel')\n",
        "    plt.title(f'Boxplot of {col}', fontsize=14)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Feature vs Target Analysis (Boxplots)\n",
        "print(\"\\nAnalyzing Feature vs Target Separation...\")\n",
        "\n",
        "# Note: Calculating RSI just for plotting comparison if needed, or use CCI/MTM\n",
        "for i, col in enumerate(indicators):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.boxplot(x='Target', y=col, data=df_all, palette='coolwarm')\n",
        "    plt.title(f'{col} Distribution by Target', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Correlation & Multicollinearity Analysis (Heatmap)\n",
        "# This section is crucial to show why PCA or feature removal is necessary\n",
        "print(\"\\nCorrelation Heatmap (Checking for Redundancy)...\")\n",
        "plt.figure(figsize=(10, 10))\n",
        "corr_matrix = df_all[indicators + target_col].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 8})\n",
        "plt.title('Correlation Matrix of ALL 11 Indicators', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Yhgy8hPUIUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing and Dimensionality Reduction**\n",
        "In this section, we transform the raw features into a format suitable for machine learning models. We perform the following steps:\n",
        "1.  Standard Scaling: Normalize all 19 features using `StandardScaler` (Mean=0, Variance=1) so that large values like Volume don't dominate the model.\n",
        "2.  Principal Component Analysis (PCA): Apply PCA to reduce the 19 correlated features into a smaller set of Principal Components that explain 95% of the variance. This removes noise and speeds up training."
      ],
      "metadata": {
        "id": "Jd09mfsW1Zb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# FEATURE SELECTION & PREPROCESSING\n",
        "# ==========================================\n",
        "\n",
        "print('=' * 65)\n",
        "print(\"--- FEATURE SELECTION & SCALING ---\")\n",
        "print('=' * 65)\n",
        "\n",
        "# Based on the above analysis, we keep all the features but remove the redundancy with PCA. This method also covers \"Task 4\" of your project.\n",
        "X = df_all[all_features]\n",
        "y = df_all['Target']\n",
        "\n",
        "# Train/Test Split (TimeSeries safe)\n",
        "split = int(len(X) * 0.8)\n",
        "X_train_raw, X_test_raw = X.iloc[:split], X.iloc[split:]\n",
        "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_test_scaled = scaler.transform(X_test_raw)\n",
        "\n",
        "# PCA (Dimensionality Reduction)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(f\"Original Features: {X_train_scaled.shape[1]}\")\n",
        "print(f\"Features after PCA: {X_train_pca.shape[1]}\")"
      ],
      "metadata": {
        "id": "OH1azfC2jGb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fit Linear SVM to find the separating plane ---\n",
        "\n",
        "# The plane equation is: w0*x + w1*y + w2*z + b = 0\n",
        "svc = SVC(kernel='linear')\n",
        "svc.fit(X_train_pca, y_train)\n",
        "\n",
        "# Extract weights and bias\n",
        "w = svc.coef_[0]\n",
        "b = svc.intercept_[0]\n",
        "\n",
        "# Generate grid for the plane\n",
        "# z = -(w0*x + w1*y + b) / w2\n",
        "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50),\n",
        "                     np.linspace(y_min, y_max, 50))\n",
        "zz = (-w[0] * xx - w[1] * yy - b) / w[2]\n",
        "\n",
        "# Plotting\n",
        "fig = plt.figure(figsize=(13, 13))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot Data Points\n",
        "# Red = Down (0), Green = Up (1)\n",
        "ax.scatter(X_train_pca[y_train==0, 0], X_train_pca[y_train==0, 1], X_train_pca[y_train==0, 2],\n",
        "           c='red', label='Down (0)', alpha=0.5, s=25, edgecolors='k', linewidth=0.2)\n",
        "ax.scatter(X_train_pca[y_train==1, 0], X_train_pca[y_train==1, 1], X_train_pca[y_train==1, 2],\n",
        "           c='green', label='Up (1)', alpha=0.5, s=25, edgecolors='k', linewidth=0.2)\n",
        "\n",
        "# Plot Hyperplane (The Blue Sheet)\n",
        "ax.plot_surface(xx, yy, zz, alpha=0.3, color='blue', shade=False)\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_zlabel('Principal Component 3')\n",
        "ax.set_title(f'3D PCA Visualization \\nBlue Plane = Linear SVM Decision Boundary', fontsize=14)\n",
        "ax.legend(loc='upper left', fontsize=12)\n",
        "ax.view_init(elev=20, azim=45) # Adjust camera angle for best view"
      ],
      "metadata": {
        "id": "PuO0naf8hFee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training and Comparison**\n",
        "In this section, we train and evaluate five different classifiers to predict stock direction. We perform the following steps:\n",
        "1.  Time Series Cross-Validation: Use `TimeSeriesSplit` to create rolling training windows (e.g., Train on Years 1-5, Test on Year 6). This prevents data leakage by ensuring we never train on future data.\n",
        "2.  Model Selection: Train Logistic Regression, KNN, SVM, Random Forest, and MLP (Neural Network).\n",
        "3.  Metric Evaluation: Measure ROC-AUC and Accuracy for each model to determine which algorithm best captures the market signal."
      ],
      "metadata": {
        "id": "6nafgbkX1hHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# MODEL TRAINING & COMPARISON\n",
        "# ==========================================\n",
        "\n",
        "print('=' * 65)\n",
        "print(\"--- MODEL TRAINING & HYPERPARAMETER TUNING ---\")\n",
        "print('=' * 65)\n",
        "\n",
        "# Define Models and Hyperparameter Grids\n",
        "# We selected 5 diverse algorithms to cover different learning styles.\n",
        "model_params = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(random_state=42, solver='liblinear'),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'penalty': ['l1', 'l2']\n",
        "        }\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'n_neighbors': [3, 5, 7, 9],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'metric': ['euclidean', 'manhattan']\n",
        "        }\n",
        "    },\n",
        "    'Support Vector Machine': {\n",
        "        'model': SVC(probability=True, random_state=42),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['rbf', 'linear'],\n",
        "            'gamma': ['scale', 'auto']\n",
        "        }\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5]\n",
        "        }\n",
        "    },\n",
        "    'MLP Neural Network': {\n",
        "        'model': MLPClassifier(max_iter=500, random_state=42),\n",
        "        'params': {\n",
        "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "            'activation': ['relu', 'tanh'],\n",
        "            'alpha': [0.0001, 0.001]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Training Loop with Time Series Cross-Validation\n",
        "# We use TimeSeriesSplit to prevent data leakage (training on future data)\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "results = []\n",
        "best_estimators = {}\n",
        "roc_data = {}\n",
        "\n",
        "print(\"Starting training loop... (This may take a few minutes)\")\n",
        "\n",
        "for model_name, config in model_params.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "    # Grid Search for Hyperparameter Tuning\n",
        "    clf = GridSearchCV(config['model'], config['params'], cv=tscv, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "    clf.fit(X_train_pca, y_train)\n",
        "\n",
        "    # Store Best Model\n",
        "    best_model = clf.best_estimator_\n",
        "    best_estimators[model_name] = best_model\n",
        "\n",
        "    # Evaluate on Test Set\n",
        "    y_pred = best_model.predict(X_test_pca)\n",
        "    y_prob = best_model.predict_proba(X_test_pca)[:, 1]\n",
        "\n",
        "    # Calculate Metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Store Results\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': acc,\n",
        "        'AUC Score': roc_auc,\n",
        "        'Best Params': str(clf.best_params_)\n",
        "    })\n",
        "    roc_data[model_name] = (fpr, tpr, roc_auc)\n",
        "\n",
        "    print(f\"  > Best Accuracy (CV): {clf.best_score_:.4f}\")\n",
        "    print(f\"  > Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"  > Test AUC: {roc_auc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yznxHPDw8zDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performance Evaluation and Visualization**\n",
        "In this section, we visually assess the trade-off between sensitivity and specificity for our models. We perform the following steps:\n",
        "1.  ROC Curve: Plot the False Positive Rate vs. True Positive Rate. An AUC score > 0.5 confirms the model performs better than random guessing.\n",
        "2.  Confusion Matrix: Generate a matrix for the best model (KNN) to analyze False Positives vs. False Negatives, helping us understand if the model is biased toward predicting \"Up\" or \"Down.\""
      ],
      "metadata": {
        "id": "iSENWatO3DPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PERFORMANCE COMPARISON & VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "print('\\n' + '=' * 65)\n",
        "print(\"--- COMPARATIVE ANALYSIS ---\")\n",
        "print('=' * 65)\n",
        "\n",
        "# Comparison Table\n",
        "results_df = pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)\n",
        "print(\"\\nModel Performance Table:\")\n",
        "print(results_df[['Model', 'Accuracy', 'AUC Score']])\n",
        "\n",
        "# Performance Bar Chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Accuracy', y='Model', hue='Model', data=results_df, palette='viridis', legend=False)\n",
        "plt.title('Model Accuracy Comparison', fontsize=15)\n",
        "plt.xlabel('Accuracy Score')\n",
        "plt.xlim(0.4, 1.0) # Adjust limit to see differences clearly\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve Comparison (Crucial for Classification)\n",
        "plt.figure(figsize=(10, 8))\n",
        "for model_name, (fpr, tpr, roc_auc) in roc_data.items():\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison', fontsize=15)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Detailed Classification Report for the Best Model\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_model_instance = best_estimators[best_model_name]\n",
        "print(f\"\\nDetailed Analysis for Best Model: {best_model_name}\")\n",
        "\n",
        "y_pred_best = best_model_instance.predict(X_test_pca)\n",
        "print(classification_report(y_test, y_pred_best))\n",
        "\n",
        "# Confusion Matrix for Best Model\n",
        "plt.figure(figsize=(6, 5))\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title(f'Confusion Matrix: {best_model_name}', fontsize=14)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFull Project Execution Completed Successfully.\")"
      ],
      "metadata": {
        "id": "FRG3NhqsUMri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ensemble Learning (Voting Classifier)**\n",
        "In this section, we combine our individual models to create a more robust predictor. We perform the following steps:\n",
        "1.  Soft Voting Strategy: Implement a `VotingClassifier` with `voting='soft'`.\n",
        "2.  Probability Aggregation: Instead of counting votes, we average the predicted probabilities of all models. This captures the confidence level of each classifier, typically resulting in higher stability and generalization."
      ],
      "metadata": {
        "id": "72GgnqI83G-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# VOTING CLASSIFIER (ENSEMBLE)\n",
        "# ==========================================\n",
        "\n",
        "print('=' * 65)\n",
        "print(\"--- ENSEMBLE LEARNING (BOOSTING PERFORMANCE) ---\")\n",
        "print('=' * 65)\n",
        "\n",
        "# 1. Retrieve best estimators from previous step\n",
        "clf1 = best_estimators['Logistic Regression']\n",
        "clf2 = best_estimators['K-Nearest Neighbors']\n",
        "clf3 = best_estimators['Support Vector Machine']\n",
        "clf4 = best_estimators['Random Forest']\n",
        "clf5 = best_estimators['MLP Neural Network']\n",
        "\n",
        "# 2. Create Voting Classifier (Soft Voting uses probabilities)\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', clf1),\n",
        "        ('knn', clf2),\n",
        "        ('svm', clf3),\n",
        "        ('rf', clf4),\n",
        "        ('mlp', clf5)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "print(\"Training Voting Classifier...\")\n",
        "voting_clf.fit(X_train_pca, y_train)\n",
        "\n",
        "# 3. Evaluate Ensemble\n",
        "y_pred_ens = voting_clf.predict(X_test_pca)\n",
        "y_prob_ens = voting_clf.predict_proba(X_test_pca)[:, 1]\n",
        "\n",
        "acc_ens = accuracy_score(y_test, y_pred_ens)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob_ens)\n",
        "roc_auc_ens = auc(fpr, tpr)\n",
        "\n",
        "print(f\"\\n[Ensemble Results]\")\n",
        "print(f\"  > Ensemble Accuracy: {acc_ens:.4f}\")\n",
        "print(f\"  > Ensemble AUC: {roc_auc_ens:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_ens))"
      ],
      "metadata": {
        "id": "fbHQLN0aBUxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Importance Analysis**\n",
        "In this section, we identify which technical indicators are driving the market predictions. We perform the following steps:\n",
        "1.  Raw Feature Training: Train a separate Random Forest Classifier on the original 19 features (without PCA) to access interpretable feature names.\n",
        "2.  Gini Importance Extraction: Extract and plot the `feature_importances` to rank indicators. This helps us verify financial theories, such as whether Volume or Momentum precedes price changes."
      ],
      "metadata": {
        "id": "G5Casioi3H_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# FEATURE IMPORTANCE ANALYSIS\n",
        "# ==========================================\n",
        "\n",
        "print('\\n' + '=' * 65)\n",
        "print(\"--- FEATURE IMPORTANCE (DIAGNOSTICS) ---\")\n",
        "print('=' * 65)\n",
        "\n",
        "# We use Random Forest to see which features actually matter\n",
        "# We must use the Random Forest trained on SCALED data (before PCA) to interpret feature names\n",
        "rf_diag = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_diag.fit(X_train_scaled, y_train)\n",
        "\n",
        "importances = rf_diag.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "# Create DataFrame for plotting\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feat_imp_df.head(10))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='magma')\n",
        "plt.title('Feature Importance (What drives the model?)', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UK4yVHHHUUe3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}